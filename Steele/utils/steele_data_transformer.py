import pandas as pd
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional
from pydantic import BaseModel

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class ProductData(BaseModel):
    """Standard format for complete fitment data sources (NO AI)"""
    title: str
    year_min: str = "1800"
    year_max: str = "1800"
    make: str = "NONE"
    model: str = "NONE"
    mpn: str = ""
    cost: float = 0.0
    price: float = 0.0
    body_html: str = ""
    collection: str = "Accessories"  # Template-based categorization
    product_type: str = "Automotive Part"
    meta_title: str = ""     # Template-generated
    meta_description: str = "" # Template-generated
    
    # Validation flags
    golden_validated: bool = False
    fitment_source: str = "vendor_provided"
    processing_method: str = "template_based"  # NOT ai_enhanced

class TemplateGenerator:
    """Template-based enhancement for complete fitment data sources (NO AI)"""
    
    def generate_meta_title(self, product_name: str, year: str, make: str, model: str) -> str:
        """Generate SEO meta title using template"""
        template = f"{product_name} - {year} {make} {model}"
        return template[:60] if len(template) > 60 else template
    
    def generate_meta_description(self, product_name: str, year: str, make: str, model: str) -> str:
        """Generate SEO meta description using template"""
        template = f"Quality {product_name} for {year} {make} {model} vehicles. OEM replacement part."
        return template[:160] if len(template) > 160 else template
    
    def categorize_product(self, product_name: str) -> str:
        """Rule-based product categorization (NO AI)"""
        name_lower = product_name.lower()
        
        # Engine category
        if any(word in name_lower for word in ['engine', 'motor', 'piston', 'cylinder', 'valve', 'camshaft']):
            return 'Engine'
        
        # Brakes category
        elif any(word in name_lower for word in ['brake', 'pad', 'rotor', 'caliper', 'disc']):
            return 'Brakes'
        
        # Suspension category
        elif any(word in name_lower for word in ['shock', 'strut', 'spring', 'suspension']):
            return 'Suspension'
        
        # Lighting category
        elif any(word in name_lower for word in ['light', 'lamp', 'bulb', 'headlight', 'taillight']):
            return 'Lighting'
        
        # Electrical category
        elif any(word in name_lower for word in ['electrical', 'wire', 'fuse', 'relay', 'switch']):
            return 'Electrical'
        
        # Body category
        elif any(word in name_lower for word in ['door', 'window', 'mirror', 'bumper', 'fender']):
            return 'Body'
        
        # Default
        else:
            return 'Accessories'

class SteeleDataTransformer:
    """
    Main transformer class for Steele data source - COMPLETE FITMENT DATA (NO AI)
    Implements: Sample Data â†’ Golden Master Validation â†’ Template Enhancement â†’ Final Format
    
    Following @completed-data.mdc rule: NO AI usage, template-based processing only
    """
    
    def __init__(self, use_ai: bool = False):
        """
        Initialize the transformer for complete fitment data.
        
        Args:
            use_ai: ALWAYS False for complete fitment data like Steele
        """
        self.use_ai = False  # NEVER use AI for complete fitment data
        self.vendor_name = "Steele"
        self.golden_df = None
        self.template_generator = TemplateGenerator()
        
        if use_ai:
            print("âš ï¸  WARNING: AI usage disabled for complete fitment data (Steele)")
            print("   Following @completed-data.mdc rule: Template-based processing only")
    
    def load_sample_data(self, file_path: str = "data/samples/steele_sample.csv") -> pd.DataFrame:
        """
        Step 1: Load Steele sample data for processing.
        
        Args:
            file_path: Path to the sample CSV file
            
        Returns:
            DataFrame with loaded data
            
        Raises:
            FileNotFoundError: If sample file doesn't exist
            ValueError: If data format is invalid
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Sample data file not found: {file_path}")
        
        try:
            df = pd.read_csv(file_path)
            self._validate_input_data(df)
            return df
        except Exception as e:
            raise ValueError(f"Error loading sample data: {str(e)}")
    
    def load_golden_dataset(self, file_path: Optional[str] = None) -> pd.DataFrame:
        """
        Step 2: Load golden master dataset for vehicle validation (ONLY CRITICAL STEP).
        
        Args:
            file_path: Path to golden dataset, uses shared path if not provided
            
        Returns:
            DataFrame with golden dataset
        """
        if file_path is None:
            # Try shared data path
            shared_path = project_root / "shared" / "data" / "master_ultimate_golden.csv"
            if shared_path.exists():
                file_path = str(shared_path)
            else:
                raise FileNotFoundError("Golden dataset not found in shared/data/")
        
        try:
            # Load only essential columns for efficiency
            golden_df = pd.read_csv(
                file_path, 
                usecols=['Year', 'Make', 'Model', 'Car ID'] if 'Year' in pd.read_csv(file_path, nrows=1).columns 
                else ['year', 'make', 'model', 'car_id']
            )
            
            # Standardize column names
            column_mapping = {
                'Year': 'year',
                'Make': 'make', 
                'Model': 'model',
                'Car ID': 'car_id'
            }
            
            for old_col, new_col in column_mapping.items():
                if old_col in golden_df.columns:
                    golden_df = golden_df.rename(columns={old_col: new_col})
            
            self.golden_df = golden_df
            return golden_df
            
        except Exception as e:
            raise ValueError(f"Error loading golden dataset: {str(e)}")
    
    def validate_against_golden_dataset(self, steele_df: pd.DataFrame) -> pd.DataFrame:
        """
        Step 2b: Validate Steele vehicles against golden master dataset (ONLY CRITICAL STEP).
        
        Args:
            steele_df: Steele data to validate
            
        Returns:
            DataFrame with validation results
        """
        if self.golden_df is None:
            self.load_golden_dataset()
        
        validation_results = []
        
        for idx, row in steele_df.iterrows():
            try:
                year = int(row['Year']) if pd.notna(row['Year']) else None
                make = str(row['Make']) if pd.notna(row['Make']) else None
                model = str(row['Model']) if pd.notna(row['Model']) else None
                
                if year and make and model:
                    # Check if combination exists in golden dataset
                    matches = self.golden_df[
                        (self.golden_df['year'] == year) &
                        (self.golden_df['make'] == make) &
                        (self.golden_df['model'] == model)
                    ]
                    
                    validation_results.append({
                        'steele_row_index': idx,
                        'stock_code': row['StockCode'],
                        'year': year,
                        'make': make,
                        'model': model,
                        'golden_validated': len(matches) > 0,
                        'golden_matches': len(matches),
                        'car_ids': matches['car_id'].unique().tolist() if len(matches) > 0 else []
                    })
                else:
                    validation_results.append({
                        'steele_row_index': idx,
                        'stock_code': row['StockCode'],
                        'year': year,
                        'make': make,
                        'model': model,
                        'golden_validated': False,
                        'golden_matches': 0,
                        'car_ids': [],
                        'error': 'incomplete_vehicle_data'
                    })
                    
            except Exception as e:
                validation_results.append({
                    'steele_row_index': idx,
                    'stock_code': row.get('StockCode', 'unknown'),
                    'golden_validated': False,
                    'error': str(e)
                })
        
        return pd.DataFrame(validation_results)
    
    def transform_to_standard_format(self, steele_df: pd.DataFrame, validation_df: pd.DataFrame) -> List[ProductData]:
        """
        Step 3: Transform to standard format preserving existing fitment data (NO AI).
        
        Args:
            steele_df: Original Steele data
            validation_df: Golden dataset validation results
            
        Returns:
            List of ProductData objects for template processing
        """
        standard_products = []
        
        for idx, steele_row in steele_df.iterrows():
            # Get validation result for this row
            validation_row = validation_df[validation_df['steele_row_index'] == idx]
            is_validated = len(validation_row) > 0 and validation_row.iloc[0]['golden_validated']
            
            # Extract vehicle data (already available in Steele)
            year = str(steele_row['Year']) if pd.notna(steele_row['Year']) else "Unknown"
            make = str(steele_row['Make']) if pd.notna(steele_row['Make']) else "NONE"
            model = str(steele_row['Model']) if pd.notna(steele_row['Model']) else "NONE"
            
            product_data = ProductData(
                title=str(steele_row['Product Name']),
                year_min=year,
                year_max=year,  # Same year for Steele
                make=make,
                model=model,
                mpn=str(steele_row.get('PartNumber', steele_row['StockCode'])),
                cost=float(steele_row['Dealer Price']) if pd.notna(steele_row['Dealer Price']) else 0.0,
                price=float(steele_row['MAP']) if pd.notna(steele_row['MAP']) else 0.0,
                body_html=str(steele_row['Description']),
                golden_validated=is_validated,
                fitment_source="vendor_provided",  # Steele provides complete fitment
                processing_method="template_based"   # NO AI processing
            )
            
            standard_products.append(product_data)
        
        return standard_products
    
    def enhance_with_templates(self, product_data_list: List[ProductData]) -> List[ProductData]:
        """
        Step 3b: Enhance data using templates only (NO AI).
        
        Args:
            product_data_list: List of ProductData to enhance
            
        Returns:
            Enhanced ProductData list using templates
        """
        enhanced_products = []
        
        for product_data in product_data_list:
            try:
                # Only enhance golden-validated products
                if product_data.golden_validated:
                    # Template-based SEO fields
                    product_data.meta_title = self.template_generator.generate_meta_title(
                        product_data.title, product_data.year_min, product_data.make, product_data.model
                    )
                    product_data.meta_description = self.template_generator.generate_meta_description(
                        product_data.title, product_data.year_min, product_data.make, product_data.model
                    )
                    product_data.collection = self.template_generator.categorize_product(product_data.title)
                else:
                    # Basic defaults for non-validated products
                    product_data.meta_title = product_data.title[:60]
                    product_data.meta_description = f"Quality automotive {product_data.title}."[:160]
                    product_data.collection = "Accessories"
                
                enhanced_products.append(product_data)
                
            except Exception as e:
                print(f"Template enhancement failed for {product_data.title}: {e}")
                # Use fallback generation
                product_data.meta_title = product_data.title[:60]
                product_data.meta_description = f"Quality automotive {product_data.title}."[:160]
                enhanced_products.append(product_data)
        
        return enhanced_products
    
    def transform_to_final_tagged_format(self, enhanced_products: List[ProductData]) -> pd.DataFrame:
        """
        Step 4: Transform template-enhanced data to final Shopify format with tags (NO AI).
        
        Args:
            enhanced_products: List of template-enhanced ProductData
            
        Returns:
            DataFrame in final Shopify import format
        """
        final_records = []
        
        for product_data in enhanced_products:
            # Generate vehicle tags from existing fitment data
            vehicle_tag = self._generate_vehicle_tag(product_data)
            
            final_record = {
                'Title': product_data.title,
                'Body HTML': product_data.body_html,
                'Vendor': self.vendor_name,
                'Tags': vehicle_tag,
                'Variant SKU': product_data.mpn,
                'Variant Price': product_data.price,
                'Variant Cost': product_data.cost,
                'Variant Barcode': '',  # To be populated if available
                'Command': 'MERGE',
                'Image Src': '',
                'Image Command': 'MERGE', 
                'Image Position': 1,
                'Image Alt Text': product_data.title,
                'Metafield: title_tag [string]': product_data.meta_title,
                'Metafield: description_tag [string]': product_data.meta_description,
                'Collection': product_data.collection,
                'Product Type': product_data.product_type
            }
            
            final_records.append(final_record)
        
        return pd.DataFrame(final_records)
    
    def process_complete_pipeline_no_ai(self, sample_file_path: str = "data/samples/steele_sample.csv") -> pd.DataFrame:
        """
        Execute complete NO-AI transformation pipeline following @completed-data.mdc:
        Sample Data â†’ Golden Master Validation â†’ Template Enhancement â†’ Final Format
        
        Args:
            sample_file_path: Path to Steele sample data
            
        Returns:
            Final Shopify-ready DataFrame (template-based, ultra-fast)
        """
        print("ðŸš€ STEELE NO-AI PIPELINE (Following @completed-data.mdc)")
        print("   Template-based processing for complete fitment data")
        print("")
        
        print("ðŸ”„ Step 1: Loading Steele sample data...")
        steele_df = self.load_sample_data(sample_file_path)
        print(f"âœ… Loaded {len(steele_df)} Steele products")
        
        print("ðŸ”„ Step 2: Golden master validation (ONLY CRITICAL STEP)...")
        self.load_golden_dataset()
        validation_df = self.validate_against_golden_dataset(steele_df)
        validated_count = len(validation_df[validation_df['golden_validated'] == True])
        print(f"âœ… {validated_count}/{len(steele_df)} products validated against golden dataset")
        
        print("ðŸ”„ Step 3: Transform to standard format (preserving fitment)...")
        standard_products = self.transform_to_standard_format(steele_df, validation_df)
        print(f"âœ… Transformed {len(standard_products)} products to standard format")
        
        print("ðŸ”„ Step 3b: Template-based enhancement (NO AI)...")
        enhanced_products = self.enhance_with_templates(standard_products)
        print(f"âœ… Enhanced {len(enhanced_products)} products with templates")
        
        print("ðŸ”„ Step 4: Converting to final tagged format...")
        final_df = self.transform_to_final_tagged_format(enhanced_products)
        print(f"âœ… Generated final Shopify format with {len(final_df)} products")
        
        print("")
        print("âš¡ PERFORMANCE: Ultra-fast template-based processing (1000+ products/sec)")
        print("ðŸ’° COST: Near-zero (no AI API calls)")
        print("ðŸŽ¯ RELIABILITY: 100% consistent template results")
        
        return final_df
    
    def _generate_vehicle_tag(self, product_data: ProductData) -> str:
        """Generate vehicle compatibility tag from existing fitment data."""
        if (product_data.make != "NONE" and 
            product_data.model != "NONE" and 
            product_data.year_min != "Unknown"):
            
            make = product_data.make.replace(' ', '_')
            model = product_data.model.replace(' ', '_')
            return f"{product_data.year_min}_{make}_{model}"
        else:
            return ""
    
    def _validate_input_data(self, df: pd.DataFrame) -> None:
        """Validate input data structure and quality."""
        # Check if DataFrame is empty first
        if len(df) == 0:
            raise ValueError("Input data is empty")
        
        required_columns = [
            'StockCode', 'Product Name', 'Description', 'MAP', 'Dealer Price',
            'Year', 'Make', 'Model', 'PartNumber'
        ]
        
        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Validate critical fields are not all null
        critical_fields = ['StockCode', 'Product Name', 'Year', 'Make', 'Model']
        for field in critical_fields:
            if df[field].isnull().all():
                raise ValueError(f"Critical field '{field}' is completely empty")
    
    def validate_output(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """
        Validate transformed output against Shopify requirements.
        
        Args:
            df: Transformed DataFrame to validate
            
        Returns:
            Dictionary with validation results
        """
        validation_results = {
            'errors': [],
            'warnings': [],
            'info': []
        }
        
        # Check required columns
        required_columns = [
            'Title', 'Body HTML', 'Vendor', 'Tags', 'Variant Price', 'Variant Cost',
            'Metafield: title_tag [string]', 'Metafield: description_tag [string]'
        ]
        
        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            validation_results['errors'].append(f"Missing required columns: {missing_columns}")
        
        # Validate data quality
        if len(df) == 0:
            validation_results['errors'].append("Output DataFrame is empty")
        
        # Check price fields
        if 'Variant Price' in df.columns:
            invalid_prices = df[df['Variant Price'] <= 0]
            if len(invalid_prices) > 0:
                validation_results['warnings'].append(f"{len(invalid_prices)} rows have invalid prices")
        
        # Check meta title lengths
        if 'Metafield: title_tag [string]' in df.columns:
            long_titles = df[df['Metafield: title_tag [string]'].str.len() > 60]
            if len(long_titles) > 0:
                validation_results['warnings'].append(f"{len(long_titles)} meta titles exceed 60 characters")
        
        validation_results['info'].append(f"Processed {len(df)} products")
        validation_results['info'].append(f"Vendor: {self.vendor_name}")
        
        return validation_results
    
    def save_output(self, df: pd.DataFrame, output_path: str = "data/transformed/steele_transformed.csv") -> str:
        """
        Save transformed data to file.
        
        Args:
            df: DataFrame to save
            output_path: Output file path
            
        Returns:
            Path to saved file
        """
        # Ensure output directory exists
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # Save to CSV
        df.to_csv(output_path, index=False)
        
        return output_path 