---
description: When there is incomplete fitment data like REM and you need to determine fitment
globs: 
alwaysApply: false
---
# Data Transformation Pipeline - Incomplete Fitment Data (REM-Type Sources)

## CORE OBJECTIVES
- Implement TDD approach for transforming vendor data sources with **INCOMPLETE FITMENT INFORMATION**
- Use AI to extract and identify year/make/model from product descriptions and titles
- Validate AI-extracted fitment data against golden master dataset
- Ensure data quality through intelligent AI processing combined with golden master validation
- Transform variable input structures into standardized format using AI assistance
- **USE ALL COLUMNS from product_import-column-requirements.py in EXACT ORDER**

## CRITICAL PRINCIPLE: INCOMPLETE FITMENT DATA HANDLING
**Key Understanding**: This rule set applies to vendors like REM who provide product data without explicit fitment columns:
- Year/Make/Model must be extracted from product descriptions using AI
- AI is REQUIRED to identify vehicle compatibility from text descriptions
- Golden master validation confirms AI-extracted fitment accuracy
- More AI usage but intelligent fitment extraction and validation
- **OUTPUT MUST INCLUDE ALL 65 COLUMNS** from shared/data/product_import/product_import-column-requirements.py

## SHOPIFY IMPORT REQUIREMENTS COMPLIANCE (FORMATTED STEP)

### MANDATORY: Complete Column Set Implementation
**CRITICAL REQUIREMENT**: All transformers MUST generate the **FORMATTED STEP** output with ALL columns from `shared/data/product_import/product_import-column-requirements.py` in the EXACT order specified, regardless of fitment completeness.

```python
# Import the complete column requirements
exec(open(str(project_root / "shared" / "data" / "product_import" / "product_import-column-requirements.py")).read())
# Now cols_list contains all 65 columns in correct order

def transform_to_formatted_shopify_import(self, enhanced_products: List[ProductDataWithAIExtraction]) -> pd.DataFrame:
    """FORMATTED STEP: Generate ALL 65 columns from product_import requirements in exact order"""
    final_records = []
    
    for product_data in enhanced_products:
        final_record = {}
        
        # Populate each column in exact order from cols_list
        for col in cols_list:
            if col == "ID":
                final_record[col] = ""  # Shopify auto-generates
            elif col == "Command":
                final_record[col] = "MERGE"
            elif col == "Title":
                final_record[col] = product_data.title
            elif col == "Body HTML":
                final_record[col] = product_data.body_html
            elif col == "Vendor":
                final_record[col] = self.vendor_name
            elif col == "Tags":
                final_record[col] = self._generate_vehicle_tag_from_ai_extraction(product_data)
            elif col == "Custom Collections":
                final_record[col] = product_data.collection  # AI-enhanced categorization
            elif col == "Variant SKU":
                final_record[col] = product_data.mpn
            elif col == "Variant Price":
                final_record[col] = product_data.price
            elif col == "Variant Cost":
                final_record[col] = product_data.cost
            elif col == "Metafield: title_tag [string]":
                final_record[col] = product_data.meta_title  # AI-generated SEO
            elif col == "Metafield: description_tag [string]":
                final_record[col] = product_data.meta_description  # AI-generated SEO
            elif col == "Metafield: custom.engine_types [list.single_line_text_field]":
                final_record[col] = self._extract_engine_types_if_applicable(product_data)
            elif col == "Variant Metafield: mm-google-shopping.mpn [single_line_text_field]":
                final_record[col] = product_data.mpn
            elif col == "Variant Metafield: mm-google-shopping.condition [single_line_text_field]":
                final_record[col] = "new"
            elif col == "Metafield: mm-google-shopping.mpn [single_line_text_field]":
                final_record[col] = product_data.mpn
            # ... map ALL other columns with appropriate defaults ...
            else:
                final_record[col] = ""  # Default empty for unmapped columns
        
        final_records.append(final_record)
    
    # Create DataFrame with columns in exact order from cols_list
    return pd.DataFrame(final_records, columns=cols_list)

def _generate_vehicle_tag_from_ai_extraction(self, product_data: ProductDataWithAIExtraction) -> str:
    """Generate vehicle tags from AI-extracted fitment data"""
    if (product_data.extraction_confidence >= 0.7 and 
        product_data.golden_validated and
        product_data.make != "UNKNOWN" and 
        product_data.model != "UNKNOWN"):
        
        make = product_data.make.replace(' ', '_')
        model = product_data.model.replace(' ', '_')
        year = product_data.year_min if product_data.year_min != "1800" else ""
        
        if year:
            return f"{year}_{make}_{model}"
        else:
            return f"{make}_{model}"
    else:
        return ""  # No tags for low-confidence or unvalidated extractions
```

### Column Categories for Incomplete Fitment Data:
- **REQUIRED_ALWAYS**: Essential columns populated with AI-extracted/enhanced data
- **REQUIRED_MULTI_VARIANTS**: For products with variants (empty for single variants)  
- **REQUIRED_MANUAL**: Columns with logical defaults (`Command: MERGE`, etc.)
- **NO_REQUIREMENTS**: Optional columns (empty unless AI extraction provides data)

### Enhanced Validation for AI-Extracted Data:
```python
def validate_output_with_ai_metadata(self, df: pd.DataFrame) -> Dict[str, List[str]]:
    """Validate output has ALL 65 columns plus AI extraction quality metrics"""
    expected_columns = cols_list  # All 65 columns from requirements
    actual_columns = list(df.columns)
    
    validation_results = {
        'errors': [],
        'warnings': [],
        'info': []
    }
    
    if actual_columns != expected_columns:
        validation_results['errors'].append("Column order/set does not match product_import requirements")
    
    # Additional validation for AI-extracted data quality
    if 'Tags' in df.columns:
        ai_tagged_count = len(df[df['Tags'] != ''])
        total_count = len(df)
        tag_rate = ai_tagged_count / total_count if total_count > 0 else 0
        
        if tag_rate < 0.3:  # Less than 30% successfully tagged
            validation_results['warnings'].append(f"Low AI extraction success rate: {tag_rate:.1%}")
        else:
            validation_results['info'].append(f"AI extraction success rate: {tag_rate:.1%}")
    
    return validation_results
```

## DATA TRANSFORMATION PIPELINE STRUCTURE

### 1. INCOMPLETE FITMENT DATA SOURCE SETUP PATTERN
For data sources with **missing or incomplete fitment information**:
```
{DataSource}/
├── data/
│   ├── samples/           # Test data WITHOUT year/make/model columns
│   ├── raw/              # Original vendor data (missing fitment)
│   ├── processed/        # AI-enhanced intermediate data
│   ├── transformed/      # Final transformed data
│   ├── formatted/        # Shopify-ready format
│   └── results/          # Output files
├── tests/
│   ├── test_data_loader.py              # Tests for basic data validation
│   ├── test_ai_fitment_extraction.py    # AI vehicle extraction tests
│   ├── test_golden_integration.py       # Golden master validation tests
│   ├── test_transformer.py             # Transformation logic tests
│   ├── test_ai_enhancement.py          # AI processing tests
│   └── test_output_format.py           # Standard output validation
├── utils/
│   ├── data_loader.py                  # Basic data loading utilities
│   ├── ai_fitment_extractor.py        # AI-powered fitment extraction
│   ├── golden_validator.py            # Golden dataset validation
│   ├── transformer.py                 # AI-enhanced transformation logic
│   └── ai_processor.py                # OpenAI integration utilities
└── src/
    └── format_{datasource}_data_to_JSON.py
```

### 2. TDD WORKFLOW FOR INCOMPLETE FITMENT DATA

#### Phase 1: Data Structure Discovery & Basic Validation Tests
```python
# test_data_loader.py - Handle missing fitment data
def test_discover_vendor_structure_without_fitment():
    """Test discovery of vendor structure without year/make/model columns"""
    # Identify product descriptions that may contain fitment info
    
def test_product_description_content_validation():
    """Test that product descriptions contain potentially extractable vehicle info"""
    # Look for patterns: years, vehicle names, part descriptions
    
def test_missing_fitment_columns_handling():
    """Test graceful handling when fitment columns are missing"""
    # Confirm we can identify when AI extraction is needed
    
def test_data_quality_without_fitment():
    """Test data quality when fitment must be extracted"""
    # Focus on description quality and extractable content
```

#### Phase 2: AI Fitment Extraction Tests
```python
# test_ai_fitment_extraction.py - Core AI extraction for missing fitment
def test_ai_extract_year_from_description():
    """Test AI extraction of vehicle years from product descriptions"""
    # Extract years like "1965-1967", "1965", "65-67" from text
    
def test_ai_extract_make_from_description():
    """Test AI extraction of vehicle makes from product descriptions"""
    # Extract makes like "Ford", "Chevrolet", "Mustang" from descriptions
    
def test_ai_extract_model_from_description():
    """Test AI extraction of vehicle models from product descriptions"""
    # Extract models considering make context
    
def test_ai_batch_fitment_extraction():
    """Test batch processing of fitment extraction"""
    # Efficient AI processing for multiple products
    
def test_ai_extraction_confidence_scoring():
    """Test confidence scoring for AI-extracted fitment data"""
    # Rate extraction confidence for validation decisions
    
def test_ai_extraction_error_handling():
    """Test handling when AI cannot extract fitment data"""
    # Graceful fallback for unclear descriptions
```

#### Phase 3: Golden Master Integration Tests with AI-Extracted Data
```python
# test_golden_integration.py - Validate AI-extracted fitment
def test_validate_ai_extracted_fitment_against_golden():
    """Test validation of AI-extracted year/make/model against golden master"""
    # Primary validation step for AI-extracted data
    
def test_ai_extraction_golden_match_rate():
    """Test that AI extractions have reasonable golden master match rates"""
    # Expect lower match rates than complete fitment data
    
def test_conflicting_ai_extractions_handling():
    """Test handling when AI extracts conflicting fitment data"""
    # Handle ambiguous or multiple vehicle compatibility
    
def test_ai_extraction_refinement_with_golden_feedback():
    """Test AI refinement based on golden master validation results"""
    # Use golden validation to improve extraction accuracy
```

#### Phase 4: AI-Enhanced Transformation Tests
```python
# test_transformer.py - Transform with AI-extracted fitment
def test_transform_with_ai_extracted_fitment():
    """Test transformation using AI-extracted year/make/model data"""
    # Build AI-friendly format from extracted fitment
    
def test_confidence_based_processing():
    """Test processing decisions based on AI extraction confidence"""
    # Higher confidence = include, lower confidence = manual review
    
def test_ai_enhanced_product_categorization():
    """Test AI categorization with extracted fitment context"""
    # Use extracted vehicle info for better categorization
    
def test_fallback_processing_for_failed_extractions():
    """Test processing when AI extraction fails"""
    # Handle products with unextractable fitment data
```

#### Phase 5: Comprehensive AI Enhancement Tests
```python
# test_ai_enhancement.py - Full AI processing pipeline
def test_ai_seo_enhancement_with_extracted_fitment():
    """Test SEO enhancement using AI-extracted fitment data"""
    # Generate meta fields with extracted vehicle info
    
def test_ai_cost_optimization():
    """Test AI usage optimization for cost efficiency"""
    # Minimize API calls while maintaining quality
    
def test_ai_processing_performance():
    """Test performance of AI-heavy processing pipeline"""
    # Monitor processing time with extensive AI usage
    
def test_ai_response_quality_validation():
    """Test quality of AI responses for fitment and enhancement"""
    # Validate AI output meets expected standards
```

#### Phase 6: Final Format & Shopify Compliance Tests
```python
# test_output_format.py - Standard output validation
def test_vehicle_tag_generation_from_ai_extracted_data():
    """Test vehicle tag generation from AI-extracted fitment"""
    # Generate year_make_model tags from AI data
    
def test_ai_confidence_metadata_inclusion():
    """Test inclusion of AI confidence scores in output metadata"""
    # Track AI extraction confidence for quality monitoring
    
def test_shopify_format_compliance_with_ai_data():
    """Test Shopify compliance when using AI-extracted fitment"""
    # Same output standards regardless of fitment source
```

### 3. INCOMPLETE FITMENT DATA HANDLING PATTERNS

#### AI-Powered Fitment Extraction:
```python
class IncompleteFitmentDataLoader:
    """Handles data sources missing fitment information"""
    
    def __init__(self, openai_client):
        self.openai_client = openai_client
        self.extraction_confidence_threshold = 0.7
    
    def extract_fitment_with_ai(self, product_description: str, product_title: str) -> Dict:
        """Extract vehicle fitment using AI from product descriptions"""
        
        extraction_prompt = f"""
        Extract vehicle fitment information from this automotive product:
        
        Title: {product_title}
        Description: {product_description[:300]}
        
        Extract:
        1. Year(s): Specific years or year ranges this part fits
        2. Make: Vehicle manufacturer (Ford, Chevrolet, etc.)
        3. Model: Specific vehicle model name
        4. Confidence: How confident you are (0.0-1.0)
        
        Return JSON format:
        {{
            "years": ["1965", "1966", "1967"],
            "make": "Ford", 
            "model": "Mustang",
            "confidence": 0.95,
            "reasoning": "Clear mention of 1965-1967 Ford Mustang in description"
        }}
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": extraction_prompt}],
                temperature=0.1
            )
            
            extracted_data = json.loads(response.choices[0].message.content)
            return self._validate_extraction(extracted_data)
            
        except Exception as e:
            return {
                "years": [],
                "make": "UNKNOWN",
                "model": "UNKNOWN", 
                "confidence": 0.0,
                "error": str(e)
            }
    
    def _validate_extraction(self, extracted_data: Dict) -> Dict:
        """Validate and normalize AI extraction results"""
        # Validate year formats
        validated_years = []
        for year in extracted_data.get("years", []):
            try:
                year_int = int(year)
                if 1900 <= year_int <= 2030:
                    validated_years.append(str(year_int))
            except:
                continue
        
        return {
            "years": validated_years,
            "make": str(extracted_data.get("make", "UNKNOWN")).strip(),
            "model": str(extracted_data.get("model", "UNKNOWN")).strip(),
            "confidence": float(extracted_data.get("confidence", 0.0)),
            "reasoning": extracted_data.get("reasoning", "")
        }
```

#### Golden Master Validation with AI-Extracted Data:
```python
def validate_ai_extracted_fitment_against_golden(extracted_fitment_list: List[Dict],
                                               golden_df: pd.DataFrame) -> pd.DataFrame:
    """Validate AI-extracted fitment against golden master"""
    validation_results = []
    
    for idx, extraction in enumerate(extracted_fitment_list):
        try:
            if extraction['confidence'] < 0.5:
                # Skip low-confidence extractions
                validation_results.append({
                    'extraction_index': idx,
                    'golden_validated': False,
                    'reason': 'low_confidence_extraction',
                    'confidence': extraction['confidence']
                })
                continue
            
            # Validate each year/make/model combination
            for year in extraction['years']:
                matches = golden_df[
                    (golden_df['year'] == int(year)) &
                    (golden_df['make'].str.upper() == extraction['make'].upper()) &
                    (golden_df['model'].str.upper() == extraction['model'].upper())
                ]
                
                validation_results.append({
                    'extraction_index': idx,
                    'year': year,
                    'make': extraction['make'],
                    'model': extraction['model'],
                    'confidence': extraction['confidence'],
                    'golden_validated': len(matches) > 0,
                    'golden_matches': len(matches),
                    'car_ids': matches['car_id'].tolist() if len(matches) > 0 else [],
                    'reasoning': extraction.get('reasoning', '')
                })
                
        except Exception as e:
            validation_results.append({
                'extraction_index': idx,
                'error': str(e),
                'golden_validated': False,
                'confidence': extraction.get('confidence', 0.0)
            })
    
    return pd.DataFrame(validation_results)
```

#### AI-Friendly Format with Extracted Fitment:
```python
class ProductDataWithAIExtraction(BaseModel):
    """AI-friendly format for incomplete fitment data sources (with complete column output)"""
    title: str
    year_min: str = "1800"
    year_max: str = "1800"
    make: str = "UNKNOWN"      # Populated by AI extraction
    model: str = "UNKNOWN"     # Populated by AI extraction
    mpn: str
    cost: float
    price: float
    body_html: str
    collection: str = "Accessories"  # AI-enhanced categorization
    product_type: str = "Automotive Part"
    meta_title: str = ""      # AI-generated SEO
    meta_description: str = "" # AI-generated SEO
    
    # AI extraction metadata
    fitment_source: str = "ai_extracted"
    extraction_confidence: float = 0.0
    ai_reasoning: str = ""
    golden_validated: bool = False
    multiple_vehicles: bool = False
    extraction_method: str = "openai_gpt4"
    
    # Complete column generation requirement
    def to_complete_shopify_format(self, vendor_name: str, cols_list: List[str]) -> Dict[str, str]:
        """Generate complete Shopify format with ALL 65 columns"""
        final_record = {}
        
        for col in cols_list:
            if col == "Title":
                final_record[col] = self.title
            elif col == "Body HTML":
                final_record[col] = self.body_html
            elif col == "Vendor":
                final_record[col] = vendor_name
            elif col == "Tags":
                final_record[col] = self._generate_ai_extracted_vehicle_tag()
            elif col == "Custom Collections":
                final_record[col] = self.collection
            elif col == "Variant SKU":
                final_record[col] = self.mpn
            elif col == "Variant Price":
                final_record[col] = self.price
            elif col == "Variant Cost":
                final_record[col] = self.cost
            elif col == "Metafield: title_tag [string]":
                final_record[col] = self.meta_title
            elif col == "Metafield: description_tag [string]":
                final_record[col] = self.meta_description
            # ... ALL other 65 columns with appropriate defaults ...
            else:
                final_record[col] = ""
        
        return final_record
    
    def _generate_ai_extracted_vehicle_tag(self) -> str:
        """Generate vehicle tags from AI-extracted fitment data"""
        if (self.extraction_confidence >= 0.7 and 
            self.golden_validated and
            self.make != "UNKNOWN" and 
            self.model != "UNKNOWN"):
            
            make = self.make.replace(' ', '_')
            model = self.model.replace(' ', '_')
            year = self.year_min if self.year_min != "1800" else ""
            
            return f"{year}_{make}_{model}" if year else f"{make}_{model}"
        else:
            return ""  # No tags for low-confidence extractions
```

### 4. VENDOR-SPECIFIC IMPLEMENTATION EXAMPLES

#### Example: REM (Incomplete Fitment Data):
```python
REM_COLUMN_MAPPING = {
    'product_name': 'Description',      # Contains fitment info
    'sku': 'Inventory ID',
    'price': ' Level-3 ',               # Note the spaces
    'description': 'Description'        # Primary source for AI extraction
}

def transform_rem_incomplete_fitment(input_file: str) -> pd.DataFrame:
    """Transform REM data with AI fitment extraction + complete 65-column output"""
    # Step 1: Load and clean vendor data
    vendor_df = pd.read_csv(input_file)
    vendor_df = vendor_df.rename(columns=REM_COLUMN_MAPPING)
    vendor_df['Price'] = vendor_df['Price'].str.strip().str.replace('$', '').astype(float)
    
    # Step 2: AI extraction of fitment from descriptions
    ai_extracted_fitment = []
    for idx, row in vendor_df.iterrows():
        extraction = extract_fitment_with_ai(
            product_description=row['Description'],
            product_title=row['Description']  # REM uses description as title
        )
        extraction['vendor_row_index'] = idx
        ai_extracted_fitment.append(extraction)
    
    # Step 3: Validate AI extractions against golden master
    golden_df = load_golden_master()
    validation_df = validate_ai_extracted_fitment_against_golden(ai_extracted_fitment, golden_df)
    
    # Step 4: Transform to AI-friendly format with extracted fitment
    ai_friendly_products = transform_to_ai_friendly_with_extracted_fitment(vendor_df, ai_extracted_fitment, validation_df)
    
    # Step 5: AI enhancement for categorization and SEO
    enhanced_products = enhance_with_comprehensive_ai(ai_friendly_products)
    
    # Step 6: Generate final format with ALL 65 columns (MANDATORY)
    final_df = transform_to_complete_column_format(enhanced_products)
    
    return final_df

def transform_to_complete_column_format(enhanced_products: List[ProductDataWithAIExtraction]) -> pd.DataFrame:
    """Generate ALL 65 columns for REM with AI-extracted fitment data"""
    # Import complete column requirements
    exec(open(str(project_root / "shared" / "data" / "product_import" / "product_import-column-requirements.py")).read())
    
    final_records = []
    
    for product_data in enhanced_products:
        # Use the model's complete format method
        final_record = product_data.to_complete_shopify_format("REM", cols_list)
        final_records.append(final_record)
    
    # Create DataFrame with ALL 65 columns in exact order
    return pd.DataFrame(final_records, columns=cols_list)
```

### 5. AI USAGE STRATEGY (COMPREHENSIVE SCOPE)

#### Fitment Extraction + Enhancement:
```python
def process_incomplete_fitment_with_ai(products: List[ProductDataWithAIExtraction]) -> List[ProductDataWithAIExtraction]:
    """Comprehensive AI processing for incomplete fitment sources"""
    enhanced_products = []
    
    for product in products:
        try:
            # Step 1: Extract fitment if not already done
            if product.fitment_source == "needs_extraction":
                fitment_extraction = extract_fitment_with_ai(
                    product.body_html, 
                    product.title
                )
                
                if fitment_extraction['confidence'] >= 0.7:
                    product.make = fitment_extraction['make']
                    product.model = fitment_extraction['model']
                    product.year_min = fitment_extraction['years'][0] if fitment_extraction['years'] else "1800"
                    product.year_max = fitment_extraction['years'][-1] if fitment_extraction['years'] else "1800"
                    product.extraction_confidence = fitment_extraction['confidence']
                    product.ai_reasoning = fitment_extraction['reasoning']
            
            # Step 2: AI enhancement for categorization and SEO
            if product.golden_validated and product.extraction_confidence >= 0.7:
                seo_enhancement = generate_comprehensive_seo_with_ai(
                    title=product.title,
                    vehicle=f"{product.year_min} {product.make} {product.model}",
                    description=product.body_html[:100],
                    confidence=product.extraction_confidence
                )
                
                product.meta_title = seo_enhancement['meta_title']
                product.meta_description = seo_enhancement['meta_description']
                product.collection = seo_enhancement.get('collection', 'Accessories')
            
            enhanced_products.append(product)
            
        except Exception as e:
            print(f"AI processing failed for {product.title}: {e}")
            # Keep original product with minimal enhancement
            product.meta_title = product.title[:60]
            product.meta_description = f"Quality automotive part: {product.title[:100]}"
            enhanced_products.append(product)
    
    return enhanced_products
```

### 6. GOLDEN MASTER INTEGRATION WORKFLOW FOR AI-EXTRACTED DATA

#### Validation Process with Confidence Scoring:
```python
def golden_master_validation_with_ai_confidence(ai_extracted_data: List[Dict]) -> pd.DataFrame:
    """Validate AI-extracted fitment with confidence-based filtering"""
    
    # Step 1: Load golden master dataset
    golden_df = load_golden_master()
    
    # Step 2: Filter by confidence threshold
    high_confidence_extractions = [
        extraction for extraction in ai_extracted_data 
        if extraction['confidence'] >= 0.6
    ]
    
    # Step 3: Validate high-confidence extractions
    validation_results = []
    for extraction in high_confidence_extractions:
        for year in extraction['years']:
            is_valid = validate_single_vehicle_in_golden(
                year, extraction['make'], extraction['model'], golden_df
            )
            
            validation_results.append({
                'extraction': extraction,
                'year': year,
                'golden_validated': is_valid,
                'confidence': extraction['confidence'],
                'action': 'include' if is_valid else 'manual_review'
            })
    
    return pd.DataFrame(validation_results)
```

### 7. REPLICATION STRATEGY FOR INCOMPLETE FITMENT SOURCES

#### Step 1: Identify Incomplete Fitment Data
```bash
# Test that vendor lacks complete fitment columns
python -c "
from utils.data_loader import check_fitment_completeness
result = check_fitment_completeness('data/samples/vendor_sample.csv')
if result['complete_fitment']:
    print('❌ Complete fitment data found - use .cursorrules instead')
else:
    print('✅ Incomplete fitment confirmed - use incomplete_fitment_data.mdc')
    print('Missing fitment columns:', result['missing_columns'])
    print('Available description columns:', result['description_columns'])
"
```

#### Step 2: Create AI Extraction Configuration
```python
# NewVendor/utils/ai_extraction_config.py for incomplete fitment sources
AI_EXTRACTION_CONFIG = {
    'description_columns': ['Description', 'ProductName', 'Title'],  # Columns to analyze
    'confidence_threshold': 0.7,  # Minimum confidence for inclusion
    'batch_size': 10,            # Products per AI request
    'extraction_model': 'gpt-4',  # AI model for extraction
    'fallback_processing': True   # Handle failed extractions
}

VENDOR_COLUMN_MAPPING = {
    'product_name': 'ProductTitle',
    'sku': 'PartNumber', 
    'price': 'RetailPrice',
    'cost': 'WholesalePrice',
    'description': 'LongDescription',  # Primary AI extraction source
    # No year/make/model columns - will be AI extracted
}
```

#### Step 3: Implementation Checklist for Incomplete Fitment
- [ ] **Confirm**: Verify year/make/model columns are missing or incomplete
- [ ] **Configure**: Set up AI extraction configuration and column mapping
- [ ] **Test**: Test AI fitment extraction with sample descriptions
- [ ] **Validate**: Test golden master validation of AI-extracted fitment
- [ ] **Transform**: Test AI-friendly format creation with extracted fitment
- [ ] **Enhance**: Test comprehensive AI processing for categorization and SEO
- [ ] **Monitor**: Test AI cost and performance monitoring
- [ ] **Output**: Validate standard Shopify format with AI-extracted vehicle tags

### 8. KEY PRINCIPLES FOR INCOMPLETE FITMENT DATA

1. **AI Extraction is Primary Data Enhancement**: Core value-add is intelligent fitment extraction
2. **Confidence-Based Processing**: Higher confidence extractions get priority processing
3. **Golden Master Validation Critical**: AI extractions must be validated against golden dataset
4. **Comprehensive AI Usage**: AI used for fitment extraction, categorization, and SEO
5. **Higher Processing Costs**: More AI usage but provides missing critical data
6. **Quality Monitoring Required**: Track AI extraction accuracy and golden match rates
7. **Fallback Processing**: Handle products where AI cannot extract fitment data
8. **Cost Optimization**: Batch processing and efficient prompting to minimize AI costs

### 9. PERFORMANCE EXPECTATIONS FOR INCOMPLETE FITMENT

- **Processing Speed**: 60-80% slower than complete fitment (extensive AI usage)
- **AI Costs**: 5-10x higher due to fitment extraction requirements
- **Extraction Success**: Expect 70-85% successful fitment extraction from descriptions
- **Golden Validation Rate**: Expect 50-70% golden master validation (lower than complete fitment)
- **Data Quality**: 80-90% accuracy with AI extraction + golden validation
- **Column Generation**: ALL 65 columns generated (same as complete fitment data)
- **Shopify Compliance**: 100% import ready with complete column set
- **Output Performance**: Minimal overhead for complete column generation (template-based mapping)

### 10. DEVELOPMENT COMMANDS FOR INCOMPLETE FITMENT

```bash
# Test AI fitment extraction capability
python -c "from utils.ai_fitment_extractor import test_extraction; test_extraction('data/samples/vendor.csv')"

# Test golden master validation with AI-extracted data
pytest tests/test_golden_integration.py -v -m "ai_extraction"

# Test comprehensive AI processing pipeline
pytest tests/test_ai_enhancement.py -v -m "ai"

# Monitor AI costs and performance
pytest tests/test_transformer.py::test_ai_cost_monitoring -v

# Test complete pipeline with AI extraction
python run_tests.py --ai-extraction-required
```

This strategy handles data sources that lack complete fitment information by using AI to extract vehicle compatibility from product descriptions, similar to the transform_REM.py approach, while maintaining the same TDD methodology and output standards.
